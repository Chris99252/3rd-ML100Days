{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業目標]\n",
    "持續接觸有關機器學習的相關專案與最新技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "透過觀察頂尖公司的機器學習文章，來了解各公司是怎麼應用機器學習在實際的專案上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業]\n",
    "今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用機器學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，這些網站都會整理最新的機器學習專案或者是技術文章，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)\n",
    "\n",
    "## 《Identifying Emotions from Walking Using Affective and Deep Features》\n",
    " \n",
    " University of Maryland : Uttaran Bhattacharya and Dinesh Manocha\n",
    " \n",
    "University of North Carolina : Tanmay Randhavane,, Kyra Kapsaskis, Kurt Gray and Aniket Bera\n",
    " \n",
    "## 專案目標 : \n",
    " \n",
    "除了語言，機器還能如何讀懂人類的情緒? \n",
    " \n",
    "在過去，研究更多集中在幫助機器解讀人類豐富表情的含義，然而近期的一些心理學文獻卻對此提出了質疑 - 很多種情況下，由於存在一些干擾，人類面部表情不一定代表著對應的交際目的。與此同時，越來越多研究表明，人體行為在情緒傳遞方面同樣扮演者非常重要的角色，而人們在行走時的身體表情或者步態，已經被證明有助於感知情緒。\n",
    " \n",
    "研究團隊提出了一種全新的自動情緒識別方法，可以將視頻中行走的人類進行歸類為快樂，悲傷，憤怒或中立4種情感類別。\n",
    " \n",
    "## 專案方法 : \n",
    " \n",
    "他們先將這些成功提取出的步態轉換為三維形態，然後使用基於LSTM（Long Short-Term Memory）的方法對這些連貫性的3D人體姿勢進行長期依賴性建模，以獲得深度特徵。接著，他們提出了表示人類行走姿勢與運動的時空情感身體特徵（最近將兩者進行集合，並使用Random Forest分類器）將成果歸類成上述提及的4種情感類別。\n",
    " \n",
    "在姿勢特徵方面，該團隊主要從這幾個方面進行了定義：\n",
    " \n",
    "體積：身體的舒展一般傳達的是正面情緒;當一個人在表達負面情緒的時候，身體姿勢往往更緊湊。\n",
    "面積：通過手和頸部之間以及腳和根關節之間的三角區域來模擬身體的擴張情況。\n",
    "距離：腳和手之間的距離也可用於模擬身體的擴張情況。\n",
    "角度：頭部傾斜情況，通過頸部不同關節延伸的角度來區分快樂和悲傷情緒。\n",
    " \n",
    "此外，他們還將步幅作為姿勢的特徵之一 - 長步幅表示憤怒和快樂;短步幅表示悲傷和中立。\n",
    " \n",
    "在運動特徵方面，他們則做出以下定義：\n",
    " \n",
    "與低喚醒情緒相比，高喚醒情緒的運動明顯在頻次上會更密集。\n",
    "快步態代表快樂或憤怒;慢步態代表悲傷。\n",
    " \n",
    "最終實驗結果顯示，該團隊的方案相比其他分類方法，準確率更高，達到80：07％;即便用於非動作數據集（非行動數據）上，準確率也高達79：72％。\n",
    " \n",
    "該團隊是第一個利用最先進的3D人體姿勢評估技術，提供能夠從步行視頻中實時識別出情感的方法。值得一提的是，這個研究最終促成了一個視頻數據集 - EWalk，內容都是些人們的行走視頻，被分別打上了對應的情感標籤。\n",
    " \n",
    "## 資料來源 : \n",
    " \n",
    "EWalk: Emotion Walk http://gamma.cs.unc.edu/GAIT/\n",
    " \n",
    "EWalk 數據集包含 1384 個具有情感標籤的步態，四種基本情緒：快樂，憤怒，悲傷和中立。這些步態可以由動作捕捉，也可以從RGB視頻中提取。使用 state-of-the-art algorithms 所產生的 synthetically generated 步態。除了為每個步態產生情感標籤，還提供情感維度的價值(效價和喚醒)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
